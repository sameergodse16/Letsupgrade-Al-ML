{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler,LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from IPython.display import Image\n",
    "from os import system\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from statistics import median,mean\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.metrics import roc_auc_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('bank-full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 features have object(string) time and 7 are integers(numbers)\n",
    "# as per the given data 10 features are of category type so lets convert them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['job','marital','education','default','housing','loan','contact','month','poutcome','Traget']]=data[['job','marital','education','default','housing','loan','contact','month','poutcome','Target']].apply(pd.Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes.to_frame('Datatypes of Attributes').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['poutcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['contact'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['job'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['housing'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Five point summary\n",
    "\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for outliners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axe= plt.subplots(nrows=4,ncols=2,figsize=(10,15))\n",
    "axe=axe.flatten()\n",
    "sns.distplot(data['age'],ax=axe[0])\n",
    "sns.distplot(data['balance'],ax=axe[1])\n",
    "sns.distplot(data['day'],ax=axe[2])\n",
    "sns.distplot(data['duration'],ax=axe[3])\n",
    "sns.distplot(data['campaign'],ax=axe[4])\n",
    "sns.distplot(data['pday'],ax=axe[5])\n",
    "sns.distplot(data['previous'],ax=axe[6])\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "axe.flat[-1].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axe= plt.subplots(nrows=4,ncols=2,figsize=(10,15))\n",
    "axe=axe.flatten()\n",
    "sns.countplot(x=data['job'],data=data,axe=axe[0])\n",
    "plt.set(plt.get_figlabels(),rotation=90)\n",
    "\n",
    "sns.countplot(x=data['marital'],data=data,ax=axe[1])\n",
    "sns.countplot(x=data['education'],data=data,ax=axe[2])\n",
    "sns.countplot(x=data['default'],data=data,ax=axe[3])\n",
    "sns.countplot(x=data['housing'],data=data,ax=axe[4])\n",
    "sns.countplot(x=data['loan'],data=data,ax=axe[5])\n",
    "sns.countplot(x=data['contact'],data=data,ax=axe[6])\n",
    "sns.countplot(x=data['month'],data=data,ax=axe[7])\n",
    "sns.countplot(x=data['poutcome'],data=data,ax=axe[8])\n",
    "sns.countplot(x=data['Traget'],data=data,ax=axe[9])\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conlusion from the graph\n",
    "\n",
    "90% of customers have no default credit\n",
    "88% customers have not subcribe for team deposit\n",
    "Maximum customers are contacted in month of May and by Cellular network\n",
    "Number of customers who have housing loan is more than other type of loans\n",
    "6% customers have credit in default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure of Skweness of Numberical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.skew().to_frame('Skewness Measures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axe= plt.subplots(nrows=4,ncols=2,figsize=(10,15))\n",
    "axe=axe.flatten()\n",
    "sns.boxplot(x='age',data=data,ax=axe[0])\n",
    "sns.boxplot(x='balance',data=data,ax=axe[1])\n",
    "sns.boxplot(x='day',data=data,ax=axe[2])\n",
    "sns.boxplot(x='duration',data=data,ax=axe[3])\n",
    "sns.boxplot(x='campaign',data=data,ax=axe[4])\n",
    "sns.boxplot(x='pdays',data=data,ax=axe[5])\n",
    "sns.boxplot(x='previous',data=data,ax=axe[6])\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "axe.flat[-1].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Traget'].value_counts().to_frame('Traget feature distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing:Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.copy()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df['default']=endocer.fit_transform(df['default']).astype(int)\n",
    "df['education']=endocer.fit_transform(df['education']).astype(int)\n",
    "df['marital']=endocer.fit_transform(df['marital']).astype(int)\n",
    "df['loan']=endocer.fit_transform(df['loan']).astype(int)\n",
    "df['housing']=endocer.fit_transform(df['housing']).astype(int)\n",
    "df['Traget']=endocer.fit_transform(df['Traget']).astype(int)\n",
    "df['poutcome']=endocer.fit_transform(df['poutcome']).astype(int)\n",
    "f['month']=endocer.fit_transform(df['month']).astype(int)\n",
    "\n",
    "data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['job','marital','education','default','housing','loan','contact','month','poutcome','Traget']]=df[['job','marital','education','default','housing','loan','contact','month','poutcome','Target']].apply(pd.Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling the Outliers with Mean replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanbal=float(df['balance'].mean())\n",
    "df['balance']=np.where(df['balance']>np.percentile)(df['balance'],75,meanbal,df['balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meandur=float(df['duration'].mean())\n",
    "df['duration']=np.where(df['duration']>np.percentile)(df['balance'],75,meandur,df['duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meancam=float(df['campaign'].mean())\n",
    "df['campaign']=np.where(df['campaign']>np.percentile)(df['campaign'],75,meancam,df['campaign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanpd=float(df['pdays'].mean())\n",
    "df['pdays']=np.where(df['pdays']>np.percentile)(df['pdays'],75,meanpd,df['pdays'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanpver=float(df['previous'].mean())\n",
    "df['previous']=np.where(df['previous']>np.percentile)(df['previous'],75,meanpver,df['previous'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After removing the Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axe= plt.subplots(nrows=4,ncols=2,figsize=(10,15))\n",
    "axe=axe.flatten()\n",
    "sns.boxplot(x='age',df=df,ax=axe[0]) # outliners are not removed\n",
    "sns.boxplot(x='balance',df=df,ax=axe[1]) # outliners are not removed\n",
    "sns.boxplot(x='day',df=df,ax=axe[2])# have no outliners\n",
    "sns.boxplot(x='duration',df=df,ax=axe[3])\n",
    "sns.boxplot(x='campaign',df=df,ax=axe[4])\n",
    "sns.boxplot(x='pdays',df=df,ax=axe[5])\n",
    "sns.boxplot(x='previous',df=df,ax=axe[6])\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "axe.flat[-1].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(df.corr(),annot=True,vmax=1,vmin=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of all features before removong the outliers\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(data.corr(),annot=True,vmax=1,vmin=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.drop(['job','education','contact','duration','pdays','day','month'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairplot of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x train,xtest,ytrain,ytest=train_test_split(x,y,test_size-0.3,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "xtrain_scale=scaler.fit_transform(xtrain)\n",
    "xtest_scale=scaler.fit_transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logic_r=LogisticRegression(solver='liblinear')\n",
    "logic_r.fit(xtrain_scaled,ytrain)\n",
    "y_pred=logic_r.predict(xtest_scaled)\n",
    "LR_accuarcy=accuary_score(ytest,y_pred)\n",
    "print('\\nConfusion martix\\n\\n',confusion_matrix(ytest,y_pred)*100)\n",
    "print('\\n Accuracy score\\n',LR_accuarcy)\n",
    "print ('\\n Classification Report \\n \\n',classification_report(ytest,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navie Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = GaussianNB()    \n",
    "NB.fit(X_train_scaled,y_train) \n",
    "y_pred = NB.predict(X_test_scaled) \n",
    "print('\\nConfusion martix\\n\\n',confusion_matrix(ytest,y_pred)*100)\n",
    "print('\\n Accuracy of Naive Bayes\\n',NB_accuracy)\n",
    "print ('\\n Classification Report \\n \\n',classification_report(ytest,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest NeighborÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)  #Instantiate KNN with k=3\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "y_pred = knn.predict(X_test_scaled) \n",
    "print('\\nConfusion martix\\n\\n',confusion_matrix(ytest,y_pred)*100)\n",
    "print('\\n Accuracy of KNN\\n',KNN_accuracy)\n",
    "print ('\\n Classification Report \\n \\n',classification_report(ytest,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train_scaled,y_train) \n",
    "predicted_svc = svc.predict(X_test_scaled) \n",
    "\n",
    "print('\\nConfusion martix\\n\\n',confusion_matrix(ytest,y_pred)*100)\n",
    "print('\\n Accuracy of SCV\\n',SVC_accuracy)\n",
    "print ('\\n Classification Report \\n \\n',classification_report(ytest,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1) \n",
    "dTree.fit(X_train, y_train) \n",
    "dt_pred= dTree.predict(X_test_scaled)\n",
    "DT_accuracy=accuracy_score(ytest,dt_pred)\n",
    "print('\\nConfusion martix\\n\\n',confusion_matrix(ytest,y_pred)*100)\n",
    "print('\\n Accuracy of Decision Tree\\n',dtt_accuracy)\n",
    "print ('\\n Classification Report \\n \\n',classification_report(ytest,dt_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance test for Decision tree\n",
    "\n",
    "xvar = df2.drop('Target', axis=1)\n",
    "feature_cols = xvar.columns\n",
    "feat_importance = dTree.tree_.compute_feature_importances(normalize=False) \n",
    "feat_imp_dict = dict(zip(feature_cols, dTree.feature_importances_))\n",
    "feat_imp = pd.data.from_dict(feat_imp_dict, orient='index')\n",
    "feat_imp.sort_values(by=0, ascending=False) \n",
    "print('\\nConfusion martix\\n\\n',confusion_matrix(ytest,y_pred)*100)\n",
    "print('\\n Accuracy of Decision Tree\\n',dtt_accuracy)\n",
    "print ('\\n Classification Report \\n \\n',classification_report(ytest,dt_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO() #string IO object\n",
    "export_graphviz(dTreeR, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['0','1']) #export graph data to dot format\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) #create graph from dot data\n",
    "graph.write_png('DT.png')#create png\n",
    "Image(graph.create_png())#display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importance = dTreeR.tree_.compute_feature_importances(normalize=False) #compute feature importance\n",
    "\n",
    "\n",
    "feat_imp_dict = dict(zip(feature_cols, dTreeR.feature_importances_)) #dictionary of columns and importance\n",
    "feat_imp = pd.data.from_dict(feat_imp_dict, orient='index') #conversion of dictionary to dataframe\n",
    "feat_imp.sort_values(by=0, ascending=False) #sort in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagc = BaggingClassifier(base_estimator=dTree, n_estimators=500,random_state=1) #Instantiate Bagging Classifier\n",
    "bagc = bagc.fit(X_train, y_train) #Call the fit method of Bagging classifier to train the model or to learn the parameters of model\n",
    "Bag_pred = bagc.predict(X_test) #Predict\n",
    "\n",
    "\n",
    "print('\\nConfusion martix\\n\\n',confusion_matrix(ytest,Bag_pred)*100)\n",
    "print('\\n Accuracy of Bagging\\n',Bagg_accuracy)\n",
    "print ('\\n Classification Report \\n \\n',classification_report(ytest,Bag_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adab = AdaBoostClassifier(n_estimators=50, random_state=1) #Instantiate Adaptive boosting Classifier\n",
    "adab = adab.fit(X_train, y_train) #Call the fit method of Adaptive boosting Classifier to train the model or to learn the parameters of model\n",
    "ADA_pred = adab.predict(X_test) #Predict\n",
    "\n",
    "\n",
    "\n",
    "print('\\nConfusion martix\\n\\n',confusion_matrix(ytest,ADA_pred)*100)\n",
    "print('\\n Accuracy of ADA\\n',ADA_accuracy)\n",
    "print ('\\n Classification Report \\n \\n',classification_report(ytest,ADA_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boostingÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradb = GradientBoostingClassifier(n_estimators = 100,random_state=1) \n",
    "gradb = gradb.fit(X_train, y_train)\n",
    "GRAD_pred = gradb.predict(X_test)\n",
    "\n",
    "\n",
    "print('\\nConfusion martix\\n\\n',confusion_matrix(ytest,GRAD_pred)*100)\n",
    "print('\\n Accuracy of GRAD\\n',GRAD_accuracy)\n",
    "print ('\\n Classification Report \\n \\n',classification_report(ytest,GRAD_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "randf = RandomForestClassifier(n_estimators = 100, random_state=1, max_features=3)\n",
    "randf = randf.fit(X_train, y_train) \n",
    "RANF_pred = randf.predict(X_test) \n",
    "\n",
    "print('\\nConfusion martix\\n\\n',confusion_matrix(ytest,RANF_pred)*100)\n",
    "print('\\n Accuracy of RF\\n',RF_accuracy)\n",
    "print ('\\n Classification Report \\n \\n',classification_report(ytest,RANF_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accurary Scores of all Models\n",
    "Scores = [('Naive bayes', NB_accuracy),\n",
    "      ('KNN', KNN_accuracy),\n",
    "      ('Logistic Regression', LR_accuracy),\n",
    "      ('SVC', SVC_accuracy ),\n",
    "      ('Decision Tree',DT_accuracy),\n",
    "      ('Bagging',BAGG_accuracy),\n",
    "      ('Adaptive Boosting',ADA_accuracy),\n",
    "      ('Gradient Boosting',GRAD_accuracy),\n",
    "      ('Random Forest N=100',RANF_accuracy),\n",
    "      \n",
    "\n",
    "Scores = pd.data(Scores,columns=['Model','Accuracy score']) \n",
    "print('\\003[1m' 'Current Scores with outliers repalced with mean:]'')\n",
    "Scores.sort_values(by='Accuracy Score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments on dataset:\n",
    "The models perform well in predicting the class 0 i.e. customer not subscribing to term deposit which can be seen in the confusion matrix of all models.\n",
    "The models do not perform well in predicting the class 1 i.e. customer subscribing to term deposit which can be seen in the confusion matrix of all models.\n",
    "Above situation occur because the Dataset is imbalanced. i.e. The ratio difference between class 0 and class 1 is huge. Which trained models to effectively identify class 0 but did not train sufficiently to classify class 1.\n",
    "This situation could have been avoided if the dataset was balanced.\n",
    "Along with imbalance, the dataset contained large number of unknown string values in 'job','education','contact' and 'poutcome' columns.\n",
    "# Comments on Models:\n",
    "When benchmarking with 'duration' column, Support Vector Classifier achieved 90% model accuracy while naive bayes score was 85% accurate.\n",
    "SVC Performed better because of it's capability of creating multiple hyperplane and then classifying the data.\n",
    "After removing the 'duration' column, The highest model score dropped by 0.5%.\n",
    "The Outliers did not affect much on accuracy scores of all models. As can be seen in above accuracy scores, getting rid of outliers by mean/median replacement did not affect the scores.\n",
    "In Decision Trees, Gradient boosting method always performed better for this dataset.\n",
    "While visualizing Decision Tree, The Pruned decision tree was easy to visualize as it had lesser leaf nodes than Tree which was not pruned.\n",
    "# Miscellaneous Comments:\n",
    "After trying get_dummies the score did not show significant difference as well as I have skipped the get_dummies step because the dataset was creating more dimension, which was making the project more computationally intensive.\n",
    "If I had kept get_dummies step, then in production stage if the new dataset turned out to be huge in number of rows then this project would have taken a lot of time to execute.\n",
    "Outliers handling did not make any significant difference in the accuracy scores of models.\n",
    "I have tried to keep minimum time complexity of this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
